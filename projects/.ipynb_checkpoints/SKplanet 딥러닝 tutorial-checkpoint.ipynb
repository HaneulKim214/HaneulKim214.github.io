{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF operations convery numpy array to Tensors automatically and Vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Hello World', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant(\"Hello World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.5, shape=(), dtype=float32)\n",
      "1.5\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1.5)\n",
    "print(a)\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10)\n",
    "\n",
    "# ???\n",
    "ds_tensors = tf.data.Dataset.from_tensor_slices(a)\n",
    "print(ds_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_ds_tensors = ds_tensors.map(tf.square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in squared_ds_tensors:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_squared_ds_tensors = ds_tensors.map(tf.square).shuffle(20).batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in shuffled_squared_ds_tensors:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "w = tf.Variable(tf.random.uniform(shape = (input_dim, output_dim)))\n",
    "\n",
    "b = tf.Variable(tf.zeros(shape=(output_dim,)))\n",
    "\n",
    "def compute_predictions(features):\n",
    "    return tf.matmul(features, w) + b\n",
    "\n",
    "def compute_loss(labels, predictions):\n",
    "    return tf.reduce_mean(tf.square(labels - predictions))\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = compute_predictions(x)\n",
    "        loss = compute_loss(y, predictions)\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [w,b])\n",
    "    w.assign_sub(learning_rate * dloss_dw)\n",
    "    b.assign_sub(learning_rate * dloss_db)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare a dataset.\n",
    "num_samples = 10000\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "    mean=[0, 3], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\n",
    "positive_samples = np.random.multivariate_normal(\n",
    "    mean=[3, 0], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\n",
    "features = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
    "labels = np.vstack((np.zeros((num_samples, 1), dtype='float32'),\n",
    "                    np.ones((num_samples, 1), dtype='float32')))\n",
    "\n",
    "plt.scatter(features[:, 0], features[:, 1], c=labels[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(features))\n",
    "features = features[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(256)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for step, (x,y) in enumerate(dataset):\n",
    "        loss = train_on_batch(x,y)\n",
    "    print(f\"Epoch {epoch}: last batch loss = {round(float(loss), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for (x,y) in (dataset):\n",
    "        loss = train_on_batch(x,y)\n",
    "t_end = time.time() - t0\n",
    "print(f\"{round(t_end/20, 3)}s per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = compute_predictions(x)\n",
    "        loss = compute_loss(y, predictions)\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [w,b])\n",
    "    w.assign_sub(learning_rate * dloss_dw)\n",
    "    b.assign_sub(learning_rate * dloss_db)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for (x,y) in (dataset):\n",
    "        loss = train_on_batch(x,y)\n",
    "t_end = time.time() - t0\n",
    "print(f\"{round(t_end/20, 3)}s per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- by using tf.function(static graphs) we can speed up our model, bigger the model greater speed benefits.\n",
    "\n",
    "- eager execution is good for debugging.\n",
    "- static graphs are good for scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5강 (CNN) Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![U-net Architecture](materials/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net Architecture\n",
    "- contraction 했다가 Expansion 하는게 segmentation 용 network의 대부분의 구조.\n",
    "- segmentation 문제는 입력과 출력의 resolution 이 같아야 한다는것. ex:1024x944 input 됬다면 1024x924 가 output 되어야함.\n",
    "- 문제는 contraction 했다가 Expansion 하게되면 위치정보를 잃어서 입력과 출력의 resolution 이 달라진다. 이것을 보존하기 위해서 grey line(??) 이 있는것.\n",
    "\n",
    "- Then why do we need to contract info then expand it? -> Our goal of segmentation figuring out context. 줄이는 이유는 전체의 context를 파악해야하기 떄문 ex: 버스앞에 여러가지 물건들이 있다면, contraction 하지않으면 물건 하나하나 segmentation 하게 됨, 하지만 우리는 \"버스다\" 를 알고 싶기 때문에 contract을 하여 전체의 context 를 이해하고 다시 expand 함.\n",
    "\n",
    "- Contraction 을 안하면 전체 context 를 파악하지 못하나 ????????????\n",
    "\n",
    "- Encoding(contraction) 때보단 Decoding(Expansion) 은 조금 덜 중요하기 때문에 computationally inexpensive 한걸 써도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load(\"oxford_iiit_pet:3.2.0\", with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6강 (RNN) Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Haneul Kim's - ARIMA</title>
	<link rel="stylesheet" href="../../../../bootstrap-4.1.3-dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="../../../../static/css/fixed.css">
    <link rel="stylesheet" href="../../../../static/css/blogs.css">
	<!-- Font Awesome libarary -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
</head>

<body data-spy="scroll" data-target="#navbarResponsive">

<!-- Navbar -->
<nav class="navbar navbar-expand-sm bg-dark navbar-dark fixed-top">
	<a class="navbar-brand" href="../../../../index.html#my-brain">
		Take me back.
    </a>
</nav>
<div class="projects">

<div class="row project">
<div class="col-md-3 side-bar">
</div>
    
<!-- ------------------------------ All Content goes here -------------------- -->
<div class="col-md-6">
    <h2> ARIMA</h1>
        
    <p class="date">date posted: 2020-10-19 </p>
    <br>
    <div class="toc">
        <h5>Contents</h5>
        <ul>
            <li>
                <a href="#1">Time series</a>
            </li>
            <li>
                <a href="#2">ARIMA</a>
                <ul>
                    <li><a href="#2-1">Parameters</a></li>
                    <li><a href="#2-2">Model building</a></li>
                    <li><a href="#2-3">Forecasting</a></li>
                    <li><a href="#2-4">Optimize parameters</a></li>
                </ul>
            </li>
            <li>
                <a href="#reference">References</a>
            </li>
        </ul>
    </div>

    <br>
    <br>

    <h5 id="1">Time series</h5>
    <p>
        Data points that are observed at specified times usually at equal intervals are referred to as time series data. Time series
        is very important in real life since most data are measured in time consequtive manner. Ex: Stock prices being recorded every second.
    </p>
    <p>
        Time series analysis are used to predict the future. For example using past 12 months sales data to predict next n month sales therefore
        we could act accordingly.
    </p>
    <p>
        Four components that explains time series data:
        <ol>
            <li><b>Trend</b> : Upward, downward, or stationary. If your company sales increase every year it is showing an upward trend.</li>
            <li><b>Seaonality</b>: Repeating pattern in certain period. Ex: difference between summer and winter. Also includes special holidays</li>
            <li><b>Irregularity</b>: External factors that affect time series data such as Covid, natural disasters.</li>
            <li><b>Cyclic</b>: repeating up and down time series data.</li>
        </ol>
    </p>


    <br><br>

    <h5 id="2">ARIMA</h5>
    <p>
        <b>A</b>uto <b>R</b>egressive <b>I</b>ntegrated <b>M</b>oving <b>A</b>verage
        <br>
        a.k.a Box-Jenkins method.
        <ul>
            <li>It is class of models that forecase using own past values: lag values and lagged forecast errors.</li>
            <li>AR model uses lag values to forecast</li>
            <li>MA model uses lagged forecast errors to forecast</li>
            <li>Two models Integrated becomes ARIMA</li>
            <li>Consists of three parameters: <b>p</b>, <b>q</b>, <b>d</b></li>
        </ul>
    </p>
    <p>
        It is a naive model since it assumes time series data are:
        <ul>
            <li>"non-seasonal" meaning different seasons do not affect its values. When there exists seasonality we use SARIMA short for Seasonal ARIMA model</li>
            <li>Has no Irregularity</li>
        </ul>
    </p>
    <br>

    
    <h6 id="2-1">Parameters</h6>
    <b>p</b> - order of AR term <br>
    <p>
        &emsp; Number of lags of Y to be used as predictors.
         In other words, If you are trying to predict June's sale how many previous(lag) month's data are you going to use?
    </p>
    <hr/>
    
    <b>q</b> - order of MA term <br>
    <p>
        &emsp; Number of lagged forecast errors -> how many past forecast errors will you use?
    </p><hr/>
    
    <b>d</b> - Minimum differncing period <br>
    <p>
        &emsp; Minimum number of differencing needed to make time series data <b>stationary</b>.
         Already stationary data would have <b>d</b> = 0.
    </p>
    <p>
        What is stationary?
        <br>
        Time series data considered stationary if it contains:
        <ol>
            <li>constant mean</li>
            <li>constant variance</li>
            <li>Covariance that is independent of time</li>
        </ol>
    </p>
    <p>
        In most cases time series data increase as time progresses therefore if you take consecutive segments it will not have constant mean.
        Below graph is Nvidia stock prices which is an example of <b>non-stationary</b> data. Segment into n periods and take means, they won't be
        the same.
    </p>
    <img src="../../../blog materials/statistics/nonstationary_data_nvidia_prices.png" alt="">
    <p>
        Stationarity is important since we need our time series data to be stationary
        before using models to forecast future. 
        <br>Often times it is non-stationary therefore we <b>difference</b> it, subtract previous value from current value.
    </p>
    <p>
        Since it is important to have stationary time series data, we need a way to test it.<br>
         Common methods of testing whether time series data is stationary are:
         <ul>
             <li>Augmented Dickey Fuller(ADF) Test</li>
             <li>Phillips-Perron(PP) Test</li>
             <li>Kwiatkowski-Phillips-Schmidt-Shin(KPSS) Test</li>
             <li>Graphing rolling statistics such as mean, standard deviation</li>
         </ul>
        If you want to delve deep into different tests read my blog on &rarr; <a href="stationary_testing.html">Stationarity Testing</a>
    </p>
    <hr />
    <h6 id="2-2">Model Building</h6>
    <p>
        We will be using python 3.8 to build ARIMA model and predict Nvidia's closing stock prices. 
    </p>
<pre>
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import yfinance as yf

def GetStockData(ticker_name, period, start_date, end_date):
    """returns Open,High,Low, and Closing price of given ticker_name from start_date to end_date""""
    tickerData = yf.Ticker(ticker_name)
    df = tickerData.history(period=period, start=start_date, end=end_date)
    return df

nvda_df = GetStockData("NVDA", "1d", "2016-01-01", "2020-10-10")
nvda_df = nvda_df[["Close"]].copy()
</pre>
    <p>
        Using yfinance library I've collected Nvidia's closing stock prices from 2010-01-01 until 2020-10-10.
    </p>
    <p>
        <img src="../../../../blog materials/statistics/nvidia_stock_data.png" alt="">
    </p>
    <p>
        I will only use Closing price and graph them to see trends, seasonality, Irregularity, and cycles.
    </p>
    <img src="../../../../blog materials/statistics/nonstationary_data_nvidia_prices.png" alt="">
    <p>
        First, we must check for its stationarity. In this case our graph clearly shows that it is not stationary
        but in some cases it may look stationary but not actually so, therefore it is always good practice to 
        check using one of three tests I've mentioned above. 
    </p>
    <p>
        In this blog I will use Augmented Dickey Fuller Test. We will use adfuller from
        statsmodel library. To test stationarity we use hypothesis testing where our 
        null hypothesis would be "time series data is non-stationary". We will reject null hypothesis
        when p-value is less than 0.05(p-value) which makes us take alternative hypothesis "time series data is stationary".
    </p>
<pre>
from statsmodels.tsa.stattools import adfuller, acf, pacf

dftest = adfuller(nvda_df["Close"], autolag="AIC") #autolag = Method to use when automatically determining the lag

dfoutput = pd.Series(dftest[0:4], index=["Test Stats", "p-value", "# Lags", "# of obs"])
for key, value in dftest[4].items():
    dfoutput[f"Critical Value ({key})"] = value
print(dfoutput)

>>> Test Stats                 1.812313
>>> p-value                    0.998373
>>> # Lags                    13.000000
>>> # of obs                1189.000000
>>> Critical Value (1%)       -3.435862
>>> Critical Value (5%)       -2.863974
>>> Critical Value (10%)      -2.568066
</pre>


    <p>
        p-value is very high therefore we cannot reject null hypothesis thus conclude that time series
        data is non-stationary. Don't give up yet, we can make it stationary by differencing our dependent
        variable.
    </p>
    <pre>
nvda_df["1st_diff"] = nvda_df["Close"].diff()
nvda_df["2nd_diff"] = nvda_df["1st_diff"].diff()

fig = make_subplots(rows=2, cols=1)

for idx, d in enumerate(["1st_diff", "2nd_diff"]):
    fig.add_trace(
        go.Scatter(
            name = d,
            x = nvda_df.index,
            y = nvda_df[d]
        ),
        row=idx+1,col=1
    )
fig.update_layout(
    title="Differnced plots"
)
fig.show()

plot_acf(nvda_df["1st_diff"].dropna());</pre>
    <p>
        Even with 1st order differencing our data became stationary.
        <img src="../../../../blog materials/statistics/nvidia_difference_plots.png" alt="">
    </p>
    <p>
        Below is autocorrelation plot of 1st order differncing. You can see that even with one lag
        it lead to negative autocorrelation right away which indicates over-differncing.
        When autocorrelation decrease too fast it may indicate over-difference and if autocorrelation
        decrease too slow(stays positive for more than 10 lags) it indicates underdifferencing.
    </p>
    <img src="../../../../blog materials/statistics/nvidia_1diff_autocorrelation.png" alt="">
    <p>
        Also when time series is <b>slightly</b> under differenced, differencing once more lead to slight
        over differencing and vice versa. In such case instead of differencing add AR terms when slightly under-differenced
        and add MA terms when slightly over-differenced.
    </p>
    <hr \>
    <h6 id="2-3">Forecasting</h6>
    <p>
        Finally time to use ARIMA model to make prediction. There is manual way to select q,d,p however
        since blog is getting too long I will explain it more deeper in later blogs and will show you 
        easy way to select parameters.
    </p>
<pre>
import pmdarima as pm

model = pm.auto_arima(nvda_df.Close, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=None,           # let model determine 'd'
                      seasonal=False,   # No Seasonality
                      start_P=0, 
                      D=0, 
                      trace=True,
                      error_action='ignore',  
                      suppress_warnings=True, 
                      stepwise=True)
print(model.summary())
</pre>
<img src="../../../../blog materials/statistics/pmd_arima_summary.png" alt="">
<br><br>
<p>
    Above code tries all combination of p,d,q and output best model which is model with lowest AIC.
    Now create best ARIMA model and make predictions. Note that since it is time series data
    <b>order matters</b> therefore must split train and test data sequentially.
</p>
<pre>
train = nvda_df.Close[:1000]
test  = nvda_df.Close[1000:]

model = ARIMA(train, order=(1, 1, 0))
fit_model = model.fit(disp=-1)

fc, se, conf = fit_model.forecast(203, alpha=0.05)  # 95% conf

fc_series    = pd.Series(fc, index=test.index)
lower_series = pd.Series(conf[:, 0], index=test.index)
upper_series = pd.Series(conf[:, 1], index=test.index)

plt.figure(figsize=(12,5), dpi=100)
plt.plot(train, label='training')
plt.plot(test, label='actual')
plt.plot(fc_series, label='forecast')
plt.fill_between(lower_series.index, lower_series, upper_series, 
                    color='k', alpha=.15)
plt.title('Forecast vs Actuals')
plt.legend(loc='upper left', fontsize=8)
plt.show()
</pre>
<img src="../../../../blog materials/statistics/ARIMA_forecast.png" alt="">
<p>
    Precition doesn't seem to good. This is because ARIMA model does not account for irregularity and since
    Nvidia price sky rocketed due to events like CES and rise of self-driving vehicles our ARIMA model did a poor job.
</p>
<p>
    Upto October 2018 there seems to be no irregularities. Lets truncate our data and see if it does well when there are
    no irregularities.
</p>
<pre>
trunc_nvda_df = nvda_df[:"2018-10-01"].copy()
train = trunc_nvda_df.Close[:600]
test  = trunc_nvda_df.Close[600:]

model = ARIMA(train, order=(1, 1, 0))
fit_model = model.fit(disp=-1)

fc, se, conf = fit_model.forecast(93, alpha=0.05)  # 95% conf

fc_series    = pd.Series(fc, index=test.index)
lower_series = pd.Series(conf[:, 0], index=test.index)
upper_series = pd.Series(conf[:, 1], index=test.index)

plt.figure(figsize=(12,5), dpi=100)
plt.plot(train, label='training')
plt.plot(test, label='actual')
plt.plot(fc_series, label='forecast')
plt.fill_between(lower_series.index, lower_series, upper_series, 
                    color='k', alpha=.15)
plt.title('Forecast vs Actuals')
plt.legend(loc='upper left', fontsize=8)
plt.show()
</pre>
<img src="../../../../blog materials/statistics/ARIMA_trunc_forecast.png" alt="">
<p>
    It does a very good job of predicting future stock prices Hooray!
</p>
<p>
    In conclusion, ARIMA is works well in simple situations where there are no irregularities and no seasonality.
    In case of irregularities and seaonality you can use SARIMAX model which stands for Seasonal ARIMA model with eXogenous variables. 
    ARIMA is a great base model if it does a great job, you use it and if it doesn't use more complex model but I always like to
    keep my model simple so I fully understand what is happening!
</p>
<p>
    Thanks for reading my blog, have a great day!
</p>
    <br>
    <div id='reference' class="references">
        <h5>References:</h5>
        <ul>
            <li><a href="https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/#:~:text=ARIMA%2C%20short%20for%20'Auto%20Regressive,used%20to%20forecast%20future%20values">Complete Guide to ARIMA </a></li>
            <li><a href="https://www.youtube.com/watch?v=e8Yw4alG16Q&t=2047s&ab_channel=edureka!">Time Series Analysis in Python </a></li>
            <li><a href="https://towardsdatascience.com/almost-everything-you-need-to-know-about-time-series-860241bdc578">Almost Everything You need to Know about time series</a></li>
            <li><a href="https://machinelearningmastery.com/time-series-data-stationary-python/">Checking stationarity in time series data with python</a></li>
        </ul>

    </div>
</body>

</html>